{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 35/35 [00:00<00:00, 423.97it/s]\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting public read permissions on data_docs objects...\n",
      "Error setting public read permissions: An error occurred (AccessDenied) when calling the PutObjectAcl operation: User: arn:aws:sts::043309357116:assumed-role/topdevs-prod-ec2-role/i-01c1fb8822e1405ff is not authorized to perform: s3:PutObjectAcl on resource: \"arn:aws:s3:::nexabrands-prod-source/data_docs/expectations/customer_data_expectation_suite.html\" because public access control lists (ACLs) are blocked by the BlockPublicAcls block public access setting.\n",
      "\n",
      "You can still access your local data docs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 35/35 [00:00<00:00, 996.14it/s] \n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website hosting is configured with index document: index.html\n",
      "Your Great Expectations Data Docs are available at: http://nexabrands-prod-gx-docs.s3-website-us-east-1.amazonaws.com/\n",
      "Created error.html page\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/utils.py:670: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 35/35 [00:00<00:00, 934.38it/s] \n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website hosting is configured with index document: index.html\n",
      "Your Great Expectations Data Docs are available at: http://nexabrands-prod-gx-docs.s3-website-us-east-1.amazonaws.com/\n",
      "Created error.html page\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.providers.amazon.aws.operators.glue import GlueJobOperator\n",
    "from airflow import DAG\n",
    "# Use the new Checkpoint Operator that supports file Data Contexts.\n",
    "from great_expectations_provider.operators.validate_checkpoint import GXValidateCheckpointOperator\n",
    "\n",
    "# Path to the Great Expectations context directory\n",
    "GX_CONTEXT_ROOT_DIR = \"/opt/airflow/include/great_expectations\"\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Environment configuration\n",
    "ENV = \"prod\"\n",
    "SOURCE_BUCKET = f\"nexabrands-{ENV}-source\"\n",
    "TARGET_BUCKET = f\"nexabrands-{ENV}-target\"\n",
    "\n",
    "# Create DAG\n",
    "dag = DAG(\n",
    "    'topdevs_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='ETL pipeline for TopDevs data processing',\n",
    "    schedule_interval='0 0 * * *',  # Daily at midnight\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['etl', 'glue', 'topdevs']\n",
    ")\n",
    "\n",
    "# Start and End dummy operators\n",
    "start = DummyOperator(task_id='start', dag=dag)\n",
    "end = DummyOperator(task_id='end', dag=dag)\n",
    "\n",
    "# Define job configurations\n",
    "job_configs = {\n",
    "    'products': {\n",
    "        'dependencies': ['start'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'products_checkpoint',\n",
    "        'suite_name': 'products_data_expectation_suite'\n",
    "    },\n",
    "    'customers': {\n",
    "        'dependencies': ['start'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'customers_checkpoint',\n",
    "        'suite_name': 'customers_data_expectation_suite'\n",
    "    },\n",
    "    'customer_targets': {\n",
    "        'dependencies': ['customers'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'customer_targets_checkpoint',\n",
    "        'suite_name': 'customer_targets_data_expectation_suite'\n",
    "    },\n",
    "    'dates': {\n",
    "        'dependencies': ['start'],\n",
    "        'validate': False\n",
    "    },\n",
    "    'orders': {\n",
    "        'dependencies': ['products', 'customers', 'dates'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'orders_checkpoint',\n",
    "        'suite_name': 'orders_data_expectation_suite'\n",
    "    },\n",
    "    'order_lines': {\n",
    "        'dependencies': ['orders'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'order_lines_checkpoint',\n",
    "        'suite_name': 'order_lines_data_expectation_suite'\n",
    "    },\n",
    "    'order_fulfillment': {\n",
    "        'dependencies': ['order_lines'],\n",
    "        'validate': True,\n",
    "        'checkpoint_name': 'order_fulfillment_checkpoint',\n",
    "        'suite_name': 'order_fulfillment_data_expectation_suite'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create Glue job tasks and corresponding validation tasks\n",
    "glue_tasks = {}\n",
    "validate_tasks = {}\n",
    "\n",
    "for job_name, config in job_configs.items():\n",
    "    task_id = f\"glue_job_{job_name}\"\n",
    "    glue_job_name = f\"topdevs-{ENV}-{job_name}-job\"\n",
    "    \n",
    "    # Define script arguments for the Glue job\n",
    "    script_args = {\n",
    "        \"--source-path\": f\"s3://{SOURCE_BUCKET}/\",\n",
    "        \"--destination-path\": f\"s3://{TARGET_BUCKET}/\",\n",
    "        \"--job-name\": glue_job_name,\n",
    "        \"--enable-auto-scaling\": \"true\",\n",
    "        \"--enable-continuous-cloudwatch-log\": \"true\",\n",
    "        \"--enable-metrics\": \"true\",\n",
    "        \"--environment\": ENV\n",
    "    }\n",
    "    \n",
    "    glue_tasks[job_name] = GlueJobOperator(\n",
    "        task_id=task_id,\n",
    "        job_name=glue_job_name,\n",
    "        script_args=script_args,\n",
    "        region_name='us-east-1',\n",
    "        wait_for_completion=True,\n",
    "        num_of_dpus=2,\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Add a validation task if configured for this job\n",
    "    if config.get('validate', False):\n",
    "        validate_task_id = f\"validate_{job_name}\"\n",
    "        checkpoint_name = config.get('checkpoint_name')\n",
    "        # Use the GXValidateCheckpointOperator for file Data Contexts.\n",
    "        validate_tasks[job_name] = GXValidateCheckpointOperator(\n",
    "            task_id=validate_task_id,\n",
    "            data_context_root_dir=GX_CONTEXT_ROOT_DIR,\n",
    "            checkpoint_name=checkpoint_name,\n",
    "            batch_parameters={\n",
    "                \"path\": f\"s3://{TARGET_BUCKET}/{job_name}/\",\n",
    "                \"datasource\": \"s3_datasource\"\n",
    "            },\n",
    "            fail_task_on_validation_failure=True,\n",
    "            return_json_dict=True,\n",
    "            context_type=\"file\",  # Specify 'file' for a File Data Context\n",
    "            dag=dag\n",
    "        )\n",
    "\n",
    "# Set up task dependencies: Glue jobs depend on the start and each other, then trigger validations and finally end the DAG.\n",
    "for job_name, config in job_configs.items():\n",
    "    for dep in config['dependencies']:\n",
    "        if dep == 'start':\n",
    "            start >> glue_tasks[job_name]\n",
    "        else:\n",
    "            glue_tasks[dep] >> glue_tasks[job_name]\n",
    "    \n",
    "    if config.get('validate', False):\n",
    "        glue_tasks[job_name] >> validate_tasks[job_name]\n",
    "        if not any(job_name in c['dependencies'] for c in job_configs.values()):\n",
    "            validate_tasks[job_name] >> end\n",
    "    else:\n",
    "        if not any(job_name in c['dependencies'] for c in job_configs.values()):\n",
    "            glue_tasks[job_name] >> end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidKeyError",
     "evalue": "Data Docs Site `s3_data_docs` already exists in the Data Context.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidKeyError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Add the S3 Data Docs site to your context\u001b[39;00m\n\u001b[32m    113\u001b[39m s3_site_name = \u001b[33m\"\u001b[39m\u001b[33ms3_data_docs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_data_docs_site\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms3_site_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msite_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms3_site_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Set up actions for the checkpoint including Data Docs update\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SlackNotificationAction, UpdateDataDocsAction\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/2024-gitops-minicamp/venv/lib/python3.12/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:955\u001b[39m, in \u001b[36mAbstractDataContext.add_data_docs_site\u001b[39m\u001b[34m(self, site_name, site_config)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.data_docs_sites \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m site_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.data_docs_sites:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m gx_exceptions.InvalidKeyError(  \u001b[38;5;66;03m# noqa: TRY003 # FIXME CoP\u001b[39;00m\n\u001b[32m    956\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData Docs Site `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` already exists in the Data Context.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    957\u001b[39m         )\n\u001b[32m    959\u001b[39m     sites = \u001b[38;5;28mself\u001b[39m.config.data_docs_sites\n\u001b[32m    960\u001b[39m     sites[site_name] = site_config\n",
      "\u001b[31mInvalidKeyError\u001b[39m: Data Docs Site `s3_data_docs` already exists in the Data Context."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing data docs site: s3_data_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics:  74%|███████▍  | 43/58 [00:00<00:00, 157.61it/s]\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n",
      "/home/ubuntu/2024-gitops-minicamp/venv/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website hosting is configured with index document: index.html\n",
      "Your Great Expectations Data Docs are available at: http://nexabrands-prod-gx-docs.s3-website-us-east-1.amazonaws.com/\n",
      "Created error.html page\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "# Set up GX context in file mode\n",
    "context = gx.get_context(mode='file',project_root_dir=\"./great_expectations\")\n",
    "\n",
    "# Use boto3 to download the data from S3\n",
    "s3_client = boto3.client('s3')\n",
    "source_bucket_name = 'nexabrands-prod-source'\n",
    "docs_bucket_name = 'nexabrands-prod-gx-docs'  # Your new dedicated GX docs bucket\n",
    "file_key = 'data/order_fulfillment.csv'\n",
    "\n",
    "response = s3_client.get_object(Bucket=source_bucket_name, Key=file_key)\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "order_fulfillment_df = pd.read_csv(StringIO(content))\n",
    "\n",
    "# Add a Pandas Data Source to your GX context\n",
    "data_source = context.data_sources.add_pandas(name=\"order_fulfillment\")\n",
    "\n",
    "# Add a Data Asset for your DataFrame\n",
    "data_asset = data_source.add_dataframe_asset(name=\"order_fulfillment\")\n",
    "\n",
    "# Define the Batch Definition name\n",
    "batch_definition_name = \"order_fulfillment_batch\"\n",
    "\n",
    "# Add the Batch Definition to the data asset\n",
    "batch_definition = data_asset.add_batch_definition_whole_dataframe(batch_definition_name)\n",
    "assert batch_definition.name == batch_definition_name\n",
    "\n",
    "# Create a new Expectation Suite\n",
    "suite = context.suites.add(gx.ExpectationSuite(name=\"order_fulfillment_data_expectation_suite\"))\n",
    "\n",
    "\n",
    "\n",
    "# Expectation 1: Check that the table has a defined set of columns (even if extra columns are allowed)\n",
    "expectation1 = gx.expectations.ExpectTableColumnsToMatchSet(\n",
    "    column_set=[\"PRODUCT_ID\", \"product.name\", \"category\"],\n",
    "    exact_match=False\n",
    ")\n",
    "\n",
    "# Expectation 2: Ensure that the PRODUCT_ID column does not contain null values and is unique\n",
    "expectation2 = gx.expectations.ExpectColumnValuesToNotBeNull(column=\"PRODUCT_ID\")\n",
    "expectation3 = gx.expectations.ExpectColumnValuesToBeUnique(column=\"PRODUCT_ID\")\n",
    "\n",
    "# Expectation 3: Ensure that the product.name column does not contain null values\n",
    "expectation4 = gx.expectations.ExpectColumnValuesToNotBeNull(column=\"product.name\")\n",
    "\n",
    "# Expectation 4: Ensure that the product.name column values have a minimum length of 1 and a maximum length of 100\n",
    "expectation5 = gx.expectations.ExpectColumnValueLengthsToBeBetween(\n",
    "    column=\"product.name\",\n",
    "    min_value=1,\n",
    "    max_value=100\n",
    ")\n",
    "\n",
    "# Expectation 5: Ensure that the category column does not contain null values\n",
    "expectation6 = gx.expectations.ExpectColumnValuesToNotBeNull(column=\"category\")\n",
    "\n",
    "# Expectation 6: Ensure that the category column values have a minimum length of 1 and a maximum length of 50\n",
    "expectation7 = gx.expectations.ExpectColumnValueLengthsToBeBetween(\n",
    "    column=\"category\",\n",
    "    min_value=1,\n",
    "    max_value=50\n",
    ")\n",
    "\n",
    "\n",
    "# Save the suite\n",
    "suite.save()\n",
    "\n",
    "# Define the Batch Parameters (the DataFrame to be validated)\n",
    "batch_parameters = {\"dataframe\": order_fulfillment_df}\n",
    "\n",
    "# Retrieve the batch\n",
    "batch = batch_definition.get_batch(batch_parameters=batch_parameters)\n",
    "\n",
    "# Create a Validation Definition\n",
    "validation_definition = gx.ValidationDefinition(\n",
    "    data=batch_definition,\n",
    "    suite=suite,\n",
    "    name='order_fulfillment'\n",
    ")\n",
    "\n",
    "# Add the Validation Definition to the context\n",
    "validation_definition = context.validation_definitions.add(validation_definition)\n",
    "\n",
    "# Configure S3 Data Docs with the new dedicated bucket\n",
    "s3_site_config = {\n",
    "    \"class_name\": \"SiteBuilder\",\n",
    "    \"site_index_builder\": {\n",
    "        \"class_name\": \"DefaultSiteIndexBuilder\",\n",
    "    },\n",
    "    \"store_backend\": {\n",
    "        \"class_name\": \"TupleS3StoreBackend\",\n",
    "        \"bucket\": docs_bucket_name,  # Using the dedicated GX docs bucket\n",
    "        \"prefix\": \"\",  # Empty prefix to put docs at the root of the bucket\n",
    "    },\n",
    "}\n",
    "\n",
    "# Check if the S3 data docs site already exists before adding it\n",
    "s3_site_name = \"s3_data_docs\"\n",
    "if s3_site_name not in context.list_data_docs_sites():\n",
    "    context.add_data_docs_site(site_name=s3_site_name, site_config=s3_site_config)\n",
    "else:\n",
    "    print(f\"Using existing data docs site: {s3_site_name}\")\n",
    "\n",
    "# Set up actions for the checkpoint including Data Docs update\n",
    "from great_expectations.checkpoint import SlackNotificationAction, UpdateDataDocsAction\n",
    "\n",
    "action_list = [\n",
    "    SlackNotificationAction(\n",
    "        name=\"Great Expectations data quality results\",\n",
    "        slack_webhook=\"https://hooks.slack.com/services/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\",\n",
    "        notify_on=\"failure\",  # Options: \"all\", \"failure\", \"success\"\n",
    "        show_failed_expectations=True,\n",
    "    ),\n",
    "    UpdateDataDocsAction(\n",
    "        name=\"update_all_data_docs\",\n",
    "        site_names=[s3_site_name],  # Specify the S3 site to update\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create checkpoint\n",
    "checkpoint = gx.Checkpoint(\n",
    "    name=\"order_fulfillment_checkpoint\",\n",
    "    validation_definitions=[validation_definition],\n",
    "    actions=action_list,\n",
    "    result_format={\"result_format\": \"COMPLETE\"},\n",
    ")\n",
    "\n",
    "# Add the checkpoint to your context and run it\n",
    "context.checkpoints.add(checkpoint)\n",
    "validation_results = checkpoint.run(batch_parameters=batch_parameters)\n",
    "\n",
    "# Build the data docs manually - this will upload the docs to S3\n",
    "context.build_data_docs(site_names=[s3_site_name])\n",
    "\n",
    "# The bucket should already be configured for website hosting via Terraform\n",
    "# Just make sure the index.html file is properly detected\n",
    "try:\n",
    "    # Verify that website hosting is enabled with expected configuration\n",
    "    website_config = s3_client.get_bucket_website(Bucket=docs_bucket_name)\n",
    "    print(f\"Website hosting is configured with index document: {website_config.get('IndexDocument', {}).get('Suffix')}\")\n",
    "    \n",
    "    # Generate the S3 website URL\n",
    "    region = s3_client.meta.region_name\n",
    "    website_url = f\"http://{docs_bucket_name}.s3-website-{region}.amazonaws.com/\"\n",
    "    print(f\"Your Great Expectations Data Docs are available at: {website_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Handle case where the bucket might not have website hosting configured yet\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Attempting to configure website hosting...\")\n",
    "    \n",
    "    try:\n",
    "        # Enable website hosting on the bucket\n",
    "        website_configuration = {\n",
    "            'ErrorDocument': {'Key': 'error.html'},\n",
    "            'IndexDocument': {'Suffix': 'index.html'},\n",
    "        }\n",
    "        s3_client.put_bucket_website(\n",
    "            Bucket=docs_bucket_name,\n",
    "            WebsiteConfiguration=website_configuration\n",
    "        )\n",
    "        \n",
    "        print(\"Website hosting successfully configured.\")\n",
    "        region = s3_client.meta.region_name\n",
    "        website_url = f\"http://{docs_bucket_name}.s3-website-{region}.amazonaws.com/\"\n",
    "        print(f\"Your Great Expectations Data Docs are available at: {website_url}\")\n",
    "        \n",
    "    except Exception as config_error:\n",
    "        print(f\"Error configuring website hosting: {config_error}\")\n",
    "        print(\"Your bucket may already be configured via Terraform, but there might be permission issues.\")\n",
    "        print(f\"Expected URL: http://{docs_bucket_name}.s3-website-{s3_client.meta.region_name}.amazonaws.com/\")\n",
    "\n",
    "# Create a simple error page if it doesn't exist\n",
    "try:\n",
    "    error_html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Error - Great Expectations Documentation</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }\n",
    "            h1 { color: #c41a16; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Page Not Found</h1>\n",
    "        <p>The requested Great Expectations documentation page was not found. Please return to the <a href=\"index.html\">home page</a>.</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_client.put_object(\n",
    "        Bucket=docs_bucket_name,\n",
    "        Key='error.html',\n",
    "        Body=error_html,\n",
    "        ContentType='text/html'\n",
    "    )\n",
    "    print(\"Created error.html page\")\n",
    "except Exception as error_page_error:\n",
    "    print(f\"Could not create error page: {error_page_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
