{"uid":"56f035678b63ca12","name":"test_clean_order_lines_data_integration","fullName":"test_order_lines#test_clean_order_lines_data_integration","historyId":"c2d965e434cf3d50d46d1fc787fe264c","time":{"start":1744632166850,"stop":1744632167149,"duration":299},"description":"Integration test for the entire data cleaning pipeline.","descriptionHtml":"<p>Integration test for the entire data cleaning pipeline.</p>\n","status":"broken","statusMessage":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","statusTrace":"spark_session = <pyspark.sql.session.SparkSession object at 0x7f2c059e5e50>\nsample_order_lines_df = DataFrame[ORDER_ID: string, PRODUCT_ID: string, ORDER_QTY: float, AGREED_DELIVERY_DATE: string, ACTUAL_DELIVERY_DATE: string, DELIVERY_QTY: string]\n\n    def test_clean_order_lines_data_integration(spark_session, sample_order_lines_df):\n        \"\"\"Integration test for the entire data cleaning pipeline.\"\"\"\n        result_df = clean_order_lines_data(sample_order_lines_df)\n        valid_order_ids = [\"ORD123\", \"ORD456\"]\n        expected_columns = [\n            \"order_id\",\n            \"product_id\",\n            \"order_qty\",\n            \"agreed_delivery_date\",\n            \"actual_delivery_date\",\n            \"delivery_qty\",\n            \"delivery_delay_days\",\n            \"delivery_completion_rate\",\n            \"is_on_time\",\n            \"is_complete_delivery\",\n        ]\n        for col_name in expected_columns:\n            assert col_name in result_df.columns\n        assert result_df.schema[\"order_id\"].dataType == StringType()\n        assert result_df.schema[\"product_id\"].dataType == IntegerType()\n        assert result_df.schema[\"order_qty\"].dataType == IntegerType()\n        assert result_df.schema[\"delivery_qty\"].dataType == IntegerType()\n        assert result_df.schema[\"agreed_delivery_date\"].dataType == DateType()\n        assert result_df.schema[\"actual_delivery_date\"].dataType == DateType()\n>       if result_df.filter(result_df.order_id == \"ORD123\").count() > 0:\n\ntest_order_lines.py:379: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count\n    return int(self._jdf.count())\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = ('xro2455', <py4j.clientserver.JavaClient object at 0x7f2c05b7af70>, 'o2454', 'count')\nkw = {}, converted = SparkUpgradeException()\n\n    def deco(*a: Any, **kw: Any) -> Any:\n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n>               raise converted from None\nE               pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nE               Fail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.\n\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185: SparkUpgradeException","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":3,"retriesStatusChange":false,"beforeStages":[{"name":"spark_session","time":{"start":1744632160650,"stop":1744632160653,"duration":3},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"sample_order_lines_df","time":{"start":1744632166840,"stop":1744632166850,"duration":10},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"afterStages":[{"name":"spark_session::0","time":{"start":1744632173108,"stop":1744632173109,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"suite","value":"test_order_lines"},{"name":"host","value":"fv-az2219-472"},{"name":"thread","value":"2959-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"test_order_lines"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[{"uid":"71423aae2070862f","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744632166326,"stop":1744632166618,"duration":292}},{"uid":"de022bb951610ce2","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744632165776,"stop":1744632166093,"duration":317}},{"uid":"b6d8638a32e598e4","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744632165172,"stop":1744632165514,"duration":342}}],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"history":{"statistic":{"failed":0,"broken":6,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"d609e1db62ceffbf","reportUrl":"https://Ajay263.github.io/2024-gitops-minicamp/19//#testresult/d609e1db62ceffbf","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744629994834,"stop":1744629995143,"duration":309}},{"uid":"7a8da2f6a6152a16","reportUrl":"https://Ajay263.github.io/2024-gitops-minicamp/18//#testresult/7a8da2f6a6152a16","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744629557070,"stop":1744629557383,"duration":313}},{"uid":"6c59d94e2200804e","reportUrl":"https://Ajay263.github.io/2024-gitops-minicamp/17//#testresult/6c59d94e2200804e","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744628972354,"stop":1744628972662,"duration":308}},{"uid":"bc369723aefda7c7","reportUrl":"https://Ajay263.github.io/2024-gitops-minicamp/17//#testresult/bc369723aefda7c7","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744627292953,"stop":1744627293267,"duration":314}},{"uid":"9b7f1e224aedec4e","reportUrl":"https://Ajay263.github.io/2024-gitops-minicamp/15//#testresult/9b7f1e224aedec4e","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744626180275,"stop":1744626180664,"duration":389}}]},"tags":[]},"source":"56f035678b63ca12.json","parameterValues":[]}