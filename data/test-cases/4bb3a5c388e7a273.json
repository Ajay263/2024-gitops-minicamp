{"uid":"4bb3a5c388e7a273","name":"test_main_with_mocked_s3","fullName":"test_products#test_main_with_mocked_s3","historyId":"7672348ac37d6ec572db503cdf963b9c","time":{"start":1737711003807,"stop":1737711004040,"duration":233},"description":"Test the main function with mocked GlueContext and S3.","descriptionHtml":"<p>Test the main function with mocked GlueContext and S3.</p>\n","status":"broken","statusMessage":"py4j.protocol.Py4JJavaError: An error occurred while calling o237.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 25 more","statusTrace":"mock_glue_context = <MagicMock name='GlueContext' id='140205445976512'>\nmock_spark_context = <MagicMock name='SparkContext' id='140205446066384'>\nmock_boto_client = <MagicMock name='client' id='140205446074320'>\nspark_session = <pyspark.sql.session.SparkSession object at 0x7f841fd67970>\n\n    @patch(\"boto3.client\")\n    @patch(\"pyspark.context.SparkContext\")\n    @patch(\"awsglue.context.GlueContext\")\n    def test_main_with_mocked_s3(\n        mock_glue_context, mock_spark_context, mock_boto_client, spark_session\n    ):\n        \"\"\"Test the main function with mocked GlueContext and S3.\"\"\"\n        # Mock GlueContext and SparkContext\n        mock_glue_context.return_value.spark_session = spark_session\n        mock_spark_context.return_value = MagicMock()\n    \n        # Mock S3 client\n        mock_s3 = mock_boto_client.return_value\n        mock_s3.list_objects_v2.return_value = {\n            \"CommonPrefixes\": [{\"Prefix\": \"products/temp/category=1/\"}]\n        }\n        mock_s3.copy_object.return_value = {}\n        mock_s3.delete_object.return_value = {}\n    \n        # Mock the Spark read chain\n        mock_df = spark_session.createDataFrame(\n            [(\"123 units\", \"Product A\", \"Category 1\")],\n            schema=[\"PRODUCT_ID\", \"product.name\", \"category\"],\n        )\n        with patch(\n            \"products.spark.read.format.return_value.option.return_value.schema.return_value.load\",\n            return_value=mock_df,\n        ):\n            # Call the main function\n>           main()\n\ntest_products.py:108: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../scripts/products.py:114: in main\n    cleaned_products.write.mode(\"overwrite\").partitionBy(\"category\").parquet(\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1721: in parquet\n    self._jwrite.parquet(path)\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__\n    return_value = get_return_value(\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco\n    return f(*a, **kw)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = 'xro238'\ngateway_client = <py4j.clientserver.JavaClient object at 0x7f841fe048e0>\ntarget_id = 'o237', name = 'parquet'\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        \"\"\"Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        \"\"\"\n        if is_error(answer)[0]:\n            if len(answer) > 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n>                   raise Py4JJavaError(\n                        \"An error occurred while calling {0}{1}{2}.\\n\".\n                        format(target_id, \".\", name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o237.parquet.\nE                   : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\nE                   \tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\nE                   \tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\nE                   \tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\nE                   \tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\nE                   \tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\nE                   \tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\nE                   \tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\nE                   \tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\nE                   \tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\nE                   \tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\nE                   \tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\nE                   \tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\nE                   \tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\nE                   \tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\nE                   \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nE                   \tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\nE                   \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\nE                   \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\nE                   \tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\nE                   \tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\nE                   \t... 25 more\n\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/py4j/protocol.py:326: Py4JJavaError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"spark_session","time":{"start":1737710998759,"stop":1737710998850,"duration":91},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"afterStages":[{"name":"spark_session::0","time":{"start":1737711004148,"stop":1737711004748,"duration":600},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"suite","value":"test_products"},{"name":"host","value":"fv-az1951-560"},{"name":"thread","value":"2422-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"test_products"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"4bb3a5c388e7a273.json","parameterValues":[]}