{"uid":"21cdd3e75e15ef8e","name":"test_clean_actual_delivery_date","fullName":"test_order_lines#test_clean_actual_delivery_date","historyId":"2f854af499113b8fb21003af1a84d70e","time":{"start":1744626176898,"stop":1744626177055,"duration":157},"description":"Test that ACTUAL_DELIVERY_DATE is properly cleaned and parsed.","descriptionHtml":"<p>Test that ACTUAL_DELIVERY_DATE is properly cleaned and parsed.</p>\n","status":"broken","statusMessage":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","statusTrace":"spark_session = <pyspark.sql.session.SparkSession object at 0x7f2be3479c40>\n\n    def test_clean_actual_delivery_date(spark_session):\n        \"\"\"Test that ACTUAL_DELIVERY_DATE is properly cleaned and parsed.\"\"\"\n        data = [\n            (\"ORD-1\", \"01/20/2024\"),  # MM/dd/yyyy format\n            (\"ORD-2\", \"2024-02-20\"),  # yyyy-MM-dd format\n            (\"ORD-3\", \"03.20.2024\"),  # Special character\n            (\"ORD-4\", \"2023-04-20\"),  # Different year, should be replaced with 2024\n            (\"ORD-5\", \"invalid-date\"),  # Invalid date\n            (\"ORD-6\", None),  # Null value\n        ]\n        schema = StructType(\n            [\n                StructField(\"ORDER_ID\", StringType(), True),\n                StructField(\"ACTUAL_DELIVERY_DATE\", StringType(), True),\n            ]\n        )\n        test_df = spark_session.createDataFrame(data=data, schema=schema)\n        result_df = clean_actual_delivery_date(test_df)\n>       rows = result_df.collect()\n\ntest_order_lines.py:209: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1263: in collect\n    sock_info = self._jdf.collectToPython()\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = ('xro426', <py4j.clientserver.JavaClient object at 0x7f2be3561b20>, 'o425', 'collectToPython')\nkw = {}, converted = SparkUpgradeException()\n\n    def deco(*a: Any, **kw: Any) -> Any:\n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n>               raise converted from None\nE               pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nE               Fail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.\n\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185: SparkUpgradeException","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":3,"retriesStatusChange":false,"beforeStages":[{"name":"spark_session","time":{"start":1744626166899,"stop":1744626169393,"duration":2494},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"afterStages":[{"name":"spark_session::0","time":{"start":1744626181544,"stop":1744626182535,"duration":991},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"suite","value":"test_order_lines"},{"name":"host","value":"fv-az1674-229"},{"name":"thread","value":"2965-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"test_order_lines"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[{"uid":"31843cd46d46766","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744626176525,"stop":1744626176680,"duration":155}},{"uid":"608b7d012d0bbd86","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744626176106,"stop":1744626176275,"duration":169}},{"uid":"a3c5fc487880b87b","status":"broken","statusDetails":"pyspark.errors.exceptions.captured.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEEE, MMMM d, yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.","time":{"start":1744626175729,"stop":1744626175888,"duration":159}}],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"21cdd3e75e15ef8e.json","parameterValues":[]}