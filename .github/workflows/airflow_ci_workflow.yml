name: Trigger Airflow DAG

permissions:
  contents: write
  pages: write
  id-token: write

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  TEST_EXECUTION_ENV: prod

jobs:
  test:
    name: 'Run Airflow Tests'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libpq-dev postgresql postgresql-contrib

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install testing dependencies
          pip install pytest pytest-cov allure-pytest pre-commit pytest-rerunfailures
          # Install Airflow and required providers
          pip install apache-airflow
          pip install apache-airflow[amazon]
          pip install apache-airflow-providers-slack
          pip install apache-airflow[statsd]
          pip install apache-airflow-providers-openlineage
          pip install apache-airflow[postgres]
          # Install additional dependencies
          pip install astronomer-cosmos
          pip install boto3
          pip install pandas
          pip install pydantic==2.8.2
          pip install psycopg2
          pip install psycopg2-binary
          # Show installed packages for debugging
          pip list

      - name: Install and run pre-commit hooks
        run: |
          pre-commit install
          SKIP=sqlfluff-lint,sqlfluff-fix,yamllint,no-commit-to-branch pre-commit run --all-files --verbose

      - name: Run Airflow tests
        id: run_tests
        continue-on-error: true
        run: |
          export PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE
          cd airflow
          export AIRFLOW_HOME=$(pwd)
          # Show contents of test directory for debugging
          echo "Test directory contents:"
          ls -la test/
          # Run pytest
          pytest test/ -v -p no:warnings --cov=dags --cov-report=xml:coverage.xml --cov-report=html:coverage_html --alluredir=allure-results

      - name: Get Allure history
        uses: actions/checkout@v4
        if: always()
        continue-on-error: true
        with:
          ref: gh-pages
          path: gh-pages

      - name: Upload Coverage Report (HTML)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage_html
          path: airflow/coverage_html

      - name: Upload Allure Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: allure-results
          path: airflow/allure-results

      - name: Generate Allure Report
        uses: simple-elf/allure-report-action@master
        if: always()
        with:
          allure_results: airflow/allure-results
          allure_history: allure-history

      - name: Upload Allure Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: allure-report
          path: allure-report

      - name: Deploy to GitHub Pages
        if: always()
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: allure-history
          keep_files: false

      - name: Set current date as env variable
        if: always()
        run: |
          echo "event_name=$GITHUB_EVENT_NAME" >> $GITHUB_OUTPUT
          echo "workflow=$GITHUB_WORKFLOW" >> $GITHUB_OUTPUT
          echo "actor=$GITHUB_ACTOR" >> $GITHUB_OUTPUT
          echo "run_number=$GITHUB_RUN_NUMBER" >> $GITHUB_OUTPUT
        id: version

      - name: Add Coverage Comment to PR
        if: github.event_name == 'pull_request' && always()
        uses: MishaKav/pytest-coverage-comment@main
        with:
          pytest-xml-coverage-path: airflow/coverage.xml
          title: Airflow DAG Test Coverage Report
          create-new-comment: true
          report-only-changed-files: true

      - name: Check test status and set job output
        if: always()
        id: check_status
        run: |
          if [ "${{ steps.run_tests.outcome }}" == "failure" ]; then
            echo "tests_failed=true" >> $GITHUB_OUTPUT
          else
            echo "tests_failed=false" >> $GITHUB_OUTPUT
          fi
    outputs:
      tests_failed: ${{ steps.check_status.outputs.tests_failed }}

  deploy-dags:
    name: 'Deploy Airflow DAGs'
    runs-on: ubuntu-latest
    environment: production
    needs: test
    defaults:
      run:
        shell: bash

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.ROLE_TO_ASSUME }}
          aws-region: us-east-1

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3

      - name: Upload DAGs to S3
        run: |
          aws s3 cp --recursive airflow/dags/ s3://${CODE_BUCKET}/dags/
        env:
          CODE_BUCKET: ${{ secrets.AIRFLOW_DAGS_BUCKET }}

      - name: Fail if tests failed
        if: needs.test.outputs.tests_failed == 'true'
        run: |
          echo "Tests failed in the previous job. Marking workflow as failed."
          exit 1
